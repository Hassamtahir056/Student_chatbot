{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12158021,"sourceType":"datasetVersion","datasetId":7657202}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =======================================================================\n# 1. SETUP AND INSTALLATIONS\n# This section installs all the necessary libraries.\n# =======================================================================\nprint(\"Step 1/9: Installing necessary libraries...\")\nimport os\n\n# The single command that installs Unsloth and all its correct dependencies.\n# This handles bitsandbytes, triton, transformers, peft, etc. automatically.\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\nprint(\"Libraries installed successfully. Please restart the runtime.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:45:43.636456Z","iopub.execute_input":"2025-06-19T11:45:43.636625Z","iopub.status.idle":"2025-06-19T11:47:18.279363Z","shell.execute_reply.started":"2025-06-19T11:45:43.636610Z","shell.execute_reply":"2025-06-19T11:47:18.278370Z"}},"outputs":[{"name":"stdout","text":"Step 1/8: Installing necessary libraries...\nCollecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-joi1l3f7/unsloth_9fb2e7574bca4183973abb53346d684d\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-joi1l3f7/unsloth_9fb2e7574bca4183973abb53346d684d\n  Resolved https://github.com/unslothai/unsloth.git to commit 37e577a91386cb5b4a7b818a1418b66beef17296\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting unsloth_zoo>=2025.5.11 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading unsloth_zoo-2025.6.1-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\nCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading tyro-0.9.24-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.51.3)\nRequirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.31.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\nCollecting bitsandbytes>=0.45.5 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.3)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.2)\nCollecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth_zoo>=2025.5.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading trl-0.18.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.0)\nCollecting cut_cross_entropy (from unsloth_zoo>=2025.5.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.1.0)\nCollecting msgspec (from unsloth_zoo>=2025.5.11->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.0)\nCollecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.18)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.4.26)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\nDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2025.6.1-py3-none-any.whl (147 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.4/147.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.9.24-py3-none-any.whl (128 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.3/128.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.18.2-py3-none-any.whl (366 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.4/366.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for unsloth: filename=unsloth-2025.6.2-py3-none-any.whl size=278541 sha256=e63b30c3fca4f72689910502568508fe50059dd74d214001f76bdeadd647583c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-y7gqyxsg/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\nSuccessfully built unsloth\nInstalling collected packages: unsloth, shtab, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, cut_cross_entropy, trl, unsloth_zoo, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.46.0 cut_cross_entropy-25.1.1 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 shtab-1.7.2 trl-0.18.2 tyro-0.9.24 unsloth-2025.6.2 unsloth_zoo-2025.6.1\nLibraries installed successfully. Please restart the runtime.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =======================================================================\n# 2. LOGINS AND ENVIRONMENT SETUP\n# Handles Hugging Face and WandB logins for model saving and tracking.\n# =======================================================================\nprint(\"\\nStep 2/9: Logging into services...\")\n\nfrom huggingface_hub import login\n\ntry:\n    # Set your Hugging Face token directly\n    login(token=hf_token)\n    \n    # If you don't want to use wandb, comment out these lines\n    import wandb\n\n    wandb.login(key=wb_token)\n    \n    # Initialize a WandB run\n    run = wandb.init(\n        project='Gemma-3-Emotion-Sensitive-Tutor',\n        job_type=\"training\",\n        anonymous=\"allow\"\n    )\n    \n    print(\"Logged into Hugging Face and WandB successfully.\")\nexcept Exception as e:\n    print(f\"Could not log in to services. Will proceed without them. Error: {e}\")\n    print(\"WARNING: Model saving to Hub and WandB tracking will be disabled.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:47:18.281771Z","iopub.execute_input":"2025-06-19T11:47:18.282007Z","iopub.status.idle":"2025-06-19T11:47:33.628938Z","shell.execute_reply.started":"2025-06-19T11:47:18.281982Z","shell.execute_reply":"2025-06-19T11:47:33.628395Z"}},"outputs":[{"name":"stdout","text":"\nStep 2/8: Logging into services...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mubaidiftikhar33\u001b[0m (\u001b[33mubaidiftikhar33-hitec-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250619_114727-zdhbtcxs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ubaidiftikhar33-hitec-university/Gemma-3-Emotion-Sensitive-Tutor/runs/zdhbtcxs?apiKey=8f4c7b3387e88b7c753aa957c5cb8fce72bcea90' target=\"_blank\">dark-frog-5</a></strong> to <a href='https://wandb.ai/ubaidiftikhar33-hitec-university/Gemma-3-Emotion-Sensitive-Tutor?apiKey=8f4c7b3387e88b7c753aa957c5cb8fce72bcea90' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ubaidiftikhar33-hitec-university/Gemma-3-Emotion-Sensitive-Tutor?apiKey=8f4c7b3387e88b7c753aa957c5cb8fce72bcea90' target=\"_blank\">https://wandb.ai/ubaidiftikhar33-hitec-university/Gemma-3-Emotion-Sensitive-Tutor?apiKey=8f4c7b3387e88b7c753aa957c5cb8fce72bcea90</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ubaidiftikhar33-hitec-university/Gemma-3-Emotion-Sensitive-Tutor/runs/zdhbtcxs?apiKey=8f4c7b3387e88b7c753aa957c5cb8fce72bcea90' target=\"_blank\">https://wandb.ai/ubaidiftikhar33-hitec-university/Gemma-3-Emotion-Sensitive-Tutor/runs/zdhbtcxs?apiKey=8f4c7b3387e88b7c753aa957c5cb8fce72bcea90</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}},{"name":"stdout","text":"Logged into Hugging Face and WandB successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =======================================================================\n# 3. LOAD THE BASE MODEL AND TOKENIZER\n# We use Unsloth's FastModel for memory efficiency and speed.\n# =======================================================================\nprint(\"\\nStep 3/9: Loading the base Gemma-3 model and tokenizer...\")\nfrom unsloth import FastModel\nimport torch\n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name=\"unsloth/gemma-3-4b-it\",\n    max_seq_length=2048,  # Suitable for conversational context\n    load_in_4bit=True,\n    token=hf_token, # Use token for gated models\n)\nprint(\"Base model and tokenizer loaded.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:47:33.629618Z","iopub.execute_input":"2025-06-19T11:47:33.629799Z","iopub.status.idle":"2025-06-19T11:48:50.232996Z","shell.execute_reply.started":"2025-06-19T11:47:33.629784Z","shell.execute_reply":"2025-06-19T11:48:50.232399Z"}},"outputs":[{"name":"stdout","text":"\nStep 3/8: Loading the base Gemma-3 model and tokenizer...\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-19 11:47:48.356672: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750333668.585875      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750333668.651039      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.6.2: Fast Gemma3 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e3dca5ae83b438ebe6d3e2eff3eaa17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ab2933af1c4902ab47284c29444efb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cabd9eb9e814b6ea9cd76e72722983c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c438ac1eb5184bf2b0c3f61fdb13a501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e81e6ec0a4f4fbeb9a0e3241214d86b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d37e7b06a8a494ca98ab9fa8b103d88"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd327e7b9e884afa9e562926574390d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc122e8b635847adb2b667b1260daf26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b59827037584bbf89dde7fefa8454a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26d525c967df42cfa3788bc263680e6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85494502c1254671adb40de334f94831"}},"metadata":{}},{"name":"stdout","text":"Base model and tokenizer loaded.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =======================================================================\n# 4. DEFINE PROMPT FORMAT AND PREPARE THE DATASET\n# This is a critical step where we format our custom data.\n# =======================================================================\nprint(\"\\nStep 4/9: Defining prompt format and processing the dataset...\")\n\n# We define a prompt template that guides the model to act as an empathetic tutor.\n# The `emotion` is explicitly included as input context.\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. Your role is to be an empathetic and encouraging tutor for a child.\n\n### Instruction:\n{}\n\n### Input:\nThe child seems to be feeling: {}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token  # End-of-sentence token is crucial for training\n\n# The function that will transform our dataset examples into the prompt format.\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    emotions = examples[\"emotion\"]\n    responses = examples[\"response\"]\n    texts = []\n    for instruction, emotion, response in zip(instructions, emotions, responses):\n        text = train_prompt_style.format(instruction, emotion, response) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load our custom dataset from the JSONL file.\nfrom datasets import load_dataset\ndataset = load_dataset(\"json\", data_files=\"/kaggle/input/tutor-dataset/tutor_dataset.jsonl\", split=\"train\")\n\n# Apply the formatting function\ndataset = dataset.map(formatting_prompts_func, batched=True)\nprint(\"Dataset successfully loaded and formatted.\")\nprint(\"Example of a formatted training sample:\\n\", dataset[0]['text'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:48:50.233790Z","iopub.execute_input":"2025-06-19T11:48:50.234425Z","iopub.status.idle":"2025-06-19T11:48:50.496778Z","shell.execute_reply.started":"2025-06-19T11:48:50.234402Z","shell.execute_reply":"2025-06-19T11:48:50.496109Z"}},"outputs":[{"name":"stdout","text":"\nStep 4/8: Defining prompt format and processing the dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2fd3cc147984a428363c2ecd8678454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220be17edce44571958cb823f0203ceb"}},"metadata":{}},{"name":"stdout","text":"Dataset successfully loaded and formatted.\nExample of a formatted training sample:\n Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. Your role is to be an empathetic and encouraging tutor for a child.\n\n### Instruction:\nI'll never get this fraction homework done! It's too hard. I hate it.\n\n### Input:\nThe child seems to be feeling: Frustration\n\n### Response:\nIt's totally okay to feel frustrated when something is tricky. Fractions can be like that at first. Let's take a deep breath. How about we look at just one problem together? Sometimes just starting with one small piece makes the whole thing feel less impossible.<end_of_turn>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =======================================================================\n# 5. CONFIGURE THE MODEL FOR FINE-TUNING (PEFT with LoRA)\n# We don't train the whole model, just small \"adapter\" layers.\n# =======================================================================\nprint(\"\\nStep 5/9: Configuring the model for PEFT/LoRA fine-tuning...\")\n\nmodel = FastModel.get_peft_model(\n    model,\n    r=16,                # LoRA rank. Higher can be more accurate but slower. 16 is a good balance.\n    lora_alpha=32,       # LoRA alpha. A good rule of thumb is 2 * r.\n    lora_dropout=0.05,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n)\nprint(\"PEFT configuration applied.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:48:50.497502Z","iopub.execute_input":"2025-06-19T11:48:50.497672Z","iopub.status.idle":"2025-06-19T11:48:57.723553Z","shell.execute_reply.started":"2025-06-19T11:48:50.497658Z","shell.execute_reply":"2025-06-19T11:48:57.722813Z"}},"outputs":[{"name":"stdout","text":"\nStep 5/8: Configuring the model for PEFT/LoRA fine-tuning...\nUnsloth: Making `base_model.model.vision_tower.vision_model` require gradients\nPEFT configuration applied.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =======================================================================\n# 6. SETUP DATA COLLATOR AND TRAINING\n# Prepare collator that masks out prompts and only computes loss on completions.\n# =======================================================================\n\nprint(\"\\nStep 6/9: Preparing collator and training configuration...\")\n\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom transformers import TrainingArguments\nfrom datasets import load_dataset\nimport torch\n\n# Fix tokenizer if it's wrapped in a processor (e.g., Gemma3Processor)\ntry:\n    from transformers import PreTrainedTokenizerBase\n    if not isinstance(tokenizer, PreTrainedTokenizerBase):\n        tokenizer = tokenizer.tokenizer\nexcept:\n    tokenizer = tokenizer.tokenizer\n\n# Load your dataset (example: alpaca)\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")  # sample size for testing\n\n# Format prompt+completion\ndef formatting_prompts(example):\n    return {\n        \"text\": f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n    }\n\ndataset = dataset.map(formatting_prompts)\n\n# Tokenize dataset\ndef tokenize(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=False, max_length=2048)\n\ntokenized_dataset = dataset.map(tokenize)\n\n# Define collator (important: this tells model to ignore loss before \"### Response:\")\nresponse_template = \"### Response:\\n\"\ncollator = DataCollatorForCompletionOnlyLM(\n    response_template=response_template,\n    tokenizer=tokenizer,\n)\n\n# Training arguments\nargs = TrainingArguments(\n    output_dir=\"gemma-3b-finetuned\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=50,  # adjust based on your goal\n    learning_rate=2e-5,\n    fp16=True,\n    logging_steps=5,\n    save_strategy=\"no\",\n    report_to=None,\n)\n\n# Create trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=tokenized_dataset,\n    args=args,\n    data_collator=collator,\n    packing=False,\n    dataset_text_field=\"text\",\n)\n\nprint(\"Trainer ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:48:57.724371Z","iopub.execute_input":"2025-06-19T11:48:57.724651Z","iopub.status.idle":"2025-06-19T11:49:01.392890Z","shell.execute_reply.started":"2025-06-19T11:48:57.724628Z","shell.execute_reply":"2025-06-19T11:49:01.392003Z"}},"outputs":[{"name":"stdout","text":"\nStep 6/8: Preparing collator and training configuration...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2b9cc0c04e246f0b87958055526ee2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dfdf540de9a492db844e17ed97ffa84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62b4b759a5524ebf956bf63d2ae0aedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cfd7de4f2f645a183e0b25d31520782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba4e5a50c1f40f98bcacab9d8b7caa3"}},"metadata":{}},{"name":"stdout","text":"Unsloth: Switching to float32 training since model cannot work with float16\nTrainer ready.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =======================================================================\n# 7. SET UP AND RUN THE SFTTRAINER\n# This is where the actual training happens.\n# =======================================================================\nprint(\"\\nStep 6/9: Setting up and starting the training process...\")\n# We need to import the correct data collator\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\nfrom transformers import TrainingArguments\n\n# Define the response template. This MUST match the start of the response section\n# in your prompt format from Step 4.\nresponse_template = \"### Response:\"\n\n# Instantiate the data collator\n# This tells the trainer to only calculate loss on the tokens that come AFTER the response_template\ncollator = DataCollatorForCompletionOnlyLM(\n    response_template=response_template,\n    tokenizer=tokenizer,\n    mlm=False, # We are doing causal language modeling, not masked language modeling\n)\n\n# NOW, we define the trainer, passing in our custom collator\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",  # We still need this to identify the text column\n    data_collator=collator,     # <-- THE KEY FIX: Pass in the custom collator\n    max_seq_length=2048,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=10,\n        # REMEMBER THE CHANGE FROM BEFORE! Use more epochs for a tiny dataset.\n        num_train_epochs=50,\n        learning_rate=2e-4,\n        logging_steps=5,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"wandb\",\n    ),\n)\n\n# Clear CUDA cache before training\nimport torch\ntorch.cuda.empty_cache()\n\n# Start training!\ntrainer_stats = trainer.train()\nprint(\"Training finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T11:49:01.394996Z","iopub.execute_input":"2025-06-19T11:49:01.395197Z","iopub.status.idle":"2025-06-19T13:09:23.282558Z","shell.execute_reply.started":"2025-06-19T11:49:01.395181Z","shell.execute_reply":"2025-06-19T13:09:23.281583Z"}},"outputs":[{"name":"stdout","text":"\nStep 6/8: Setting up and starting the training process...\nUnsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d50f6919580a440aa70f63b598718662"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 50 | Total steps = 1,550\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 38,497,792/4,000,000,000 (0.96% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='469' max='1550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 469/1550 1:19:05 < 3:03:04, 0.10 it/s, Epoch 14.64/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>1.603600</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.176100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.051700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.022300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.006000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.046700</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.921000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.825700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.879100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.755300</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.772800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.892100</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.785800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.535600</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.488200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.587100</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.480500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.495400</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.558300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.313800</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.275700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.315600</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.255700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.271700</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.265700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.212900</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.145400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.175400</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.163500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.148300</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.168500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.152100</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.081400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.075500</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.103600</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.105100</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.063600</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.090300</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.072400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.054800</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.036200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.062000</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.057600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.082500</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.040500</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.028800</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.030000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.037900</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.024100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.034900</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.033200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.014100</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.017200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.020500</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.018800</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.018100</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.020400</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.010900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.008200</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>0.005900</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.021100</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.004900</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.005500</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.007400</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.001700</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>0.000900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.005500</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.001800</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>0.002600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.000400</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>0.001300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.000500</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.000400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4231335245.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Start training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrainer_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/compiler.py\u001b[0m in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:09:23.283120Z","iopub.status.idle":"2025-06-19T13:09:23.283419Z","shell.execute_reply.started":"2025-06-19T13:09:23.283274Z","shell.execute_reply":"2025-06-19T13:09:23.283294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================================================\n# 8. TEST THE FINE-TUNED MODEL\n# Let's see if the model learned to be an empathetic tutor.\n# =======================================================================\nprint(\"\\nStep 7/8: Testing the fine-tuned model with sample prompts...\")\n\n# We create a slightly different prompt for inference (without the response part).\ninference_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. Your role is to be an empathetic and encouraging tutor for a child.\n\n### Instruction:\n{}\n\n### Input:\nThe child seems to be feeling: {}\n\n### Response:\n\"\"\"\n\nFastModel.for_inference(model)  # Prepare the model for fast inference\n\n# Test case 1: Frustration with math\ninstruction = \"I can't do this, I'm too dumb for division!\"\nemotion = \"Frustration / low self-esteem\"\ninputs = tokenizer([inference_prompt_style.format(instruction, emotion)], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=512, use_cache=True)\nresponse_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(\"--- Test Case 1: Frustration ---\")\nprint(response_text)\n\n# Test case 2: Boredom with history\ninstruction = \"Why do I have to learn about old boring stuff?\"\nemotion = \"Boredom\"\ninputs = tokenizer([inference_prompt_style.format(instruction, emotion)], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=512, use_cache=True)\nresponse_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\nprint(\"\\n--- Test Case 2: Boredom ---\")\nprint(response_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:09:54.112886Z","iopub.execute_input":"2025-06-19T13:09:54.113414Z","iopub.status.idle":"2025-06-19T13:14:24.302701Z","shell.execute_reply.started":"2025-06-19T13:09:54.113388Z","shell.execute_reply":"2025-06-19T13:14:24.301980Z"}},"outputs":[{"name":"stdout","text":"\nStep 7/8: Testing the fine-tuned model with sample prompts...\n--- Test Case 1: Frustration ---\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. Your role is to be an empathetic and encouraging tutor for a child.\n\n### Instruction:\nI can't do this, I'm too dumb for division!\n\n### Input:\nThe child seems to be feeling: Frustration / low self-esteem\n\n### Response:\nOh, honey, I understand that you're feeling frustrated and like you're not good at division. But please don't say that - you're not dumb, and it's okay to struggle with some things. Division can be tricky, and it's not something that everyone learns at the same pace. The important thing is that you keep trying, and that you don't give up on yourself. Remember, I'm here to help you, and we can work through this together, step by step, until you understand it. Just like how we learn to walk, we also learn through practice and persistence. And please know that your worth as a person is not determined by your ability to do division. You are valuable and capable, and I believe in you! Let's take a deep breath and see if we can make this a little easier for you. How about we start by going over the basics of division, and then move on to some easier problems, and then we can work our way up? What do you think?\nIn the meantime, would you like me to explain division in a different way, or would you like to take a break and come back to it later?\nRemember, I'm here to help you every step of the way. Don't be afraid to ask for help, and please believe in yourself. You can do this! Just keep trying, and eventually, you'll get it. And even if you don't get it right away, that's okay too - we can still learn and grow from the struggle. The important thing is that you never stop trying to learn and to understand the world around you. And please, don't let this one challenge define you or your abilities. You are much more than just your performance in division. You are a wonderful, valuable person, and I am here to support you in all of your endeavors. Let's work together, and let's make learning an enjoyable journey for you. What do you say? Shall we take it one step at a time?\nI am here to listen to your concerns and to help you build confidence in your abilities. You are not alone in this, and I am here to be your guide and your friend. Let's do this together, and let's make learning fun again! How about we start by talking about what makes you feel frustrated when you're doing division? That way, we can find ways to address those specific challenges and help\n\n--- Test Case 2: Boredom ---\nBelow is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. Your role is to be an empathetic and encouraging tutor for a child.\n\n### Instruction:\nWhy do I have to learn about old boring stuff?\n\n### Input:\nThe child seems to be feeling: Boredom\n\n### Response:\nOh, wow, that's a great question! It might seem boring at first, but learning about the past can actually be super interesting. Think of it like this - history helps us understand where we came from and how the world became the way it is today. It's like having a roadmap of all the amazing things that have happened! Plus, learning about old stuff can help us see how far we've come and what we can achieve. It gives us perspective, you know? And who knows, maybe some of those old boring topics will even spark your interest in something new and exciting! Just give it a chance and you never know what discoveries you might make. And hey, if it's still boring, that's okay too! We can always find something else to learn about that might be more up your alley. The important thing is that you're curious and want to grow and learn, and that's already a great start. Is there anything specific that makes you feel like history is boring? Maybe we can try to find a way to make it more engaging for you.\nWould you like to talk more about that, or would you like to move on to a different topic? Whatever you need, I'm here to help you learn and grow. Just remember, you've got this, and I'm here to support you every step of the way. Let's work together to make learning an adventure! How about we try to find some cool facts or stories from history, or even look at some pictures or videos to see if that helps? Or, if you'd rather, we can talk about something you're more interested in right now. The choice is yours, and I'm here to help you in any way I can. Just let me know what you need. And remember, even if it seems like a small step, every little bit of learning helps you grow and become more amazing! So, let's take that step together, and see where it leads us. What do you say? Would you like to try something new, or would you rather stick with what you know? Either way is fine, and we'll go at your pace. Just relax, and let's see what wonders we can discover together. And hey, if we learn something new, that's great! But if not, that's okay too. We still had fun, and that's what matters most. Just remember, I'm here to help\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:09:23.285827Z","iopub.status.idle":"2025-06-19T13:09:23.286051Z","shell.execute_reply.started":"2025-06-19T13:09:23.285945Z","shell.execute_reply":"2025-06-19T13:09:23.285955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:09:23.286961Z","iopub.status.idle":"2025-06-19T13:09:23.287281Z","shell.execute_reply.started":"2025-06-19T13:09:23.287119Z","shell.execute_reply":"2025-06-19T13:09:23.287134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =======================================================================\n# 9. SAVE THE FINAL MODEL\n# Save the trained model adapters locally and push to Hugging Face Hub.\n# =======================================================================\nprint(\"\\nStep 9/9: Saving the model...\")\n\n# Choose a name for your model on the Hub\nnew_model_name_online = \"Gemma-3-Emotion-Sensitive-Tutor-v1\" \n# And a local directory name\nnew_model_name_local = \"Gemma-3-Tutor-local\"\n\n# Save LoRA adapters locally\nmodel.save_pretrained(new_model_name_local)\ntokenizer.save_pretrained(new_model_name_local)\nprint(f\"Model saved locally to '{new_model_name_local}'\")\n\n# Push to Hugging Face Hub (if logged in)\ntry:\n    model.push_to_hub(new_model_name_online, token=hf_token)\n    tokenizer.push_to_hub(new_model_name_online, token=hf_token)\n    print(f\"Model pushed to Hugging Face Hub as '{new_model_name_online}'\")\nexcept Exception as e:\n    print(f\"Could not push to Hub. Please ensure you are logged in and the repo doesn't exist. Error: {e}\")\n\nif 'run' in locals() and run is not None:\n    wandb.finish()\n\nprint(\"\\n🎉 All steps completed! 🎉\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T13:19:46.494791Z","iopub.execute_input":"2025-06-19T13:19:46.495657Z","iopub.status.idle":"2025-06-19T13:19:50.056860Z","shell.execute_reply.started":"2025-06-19T13:19:46.495626Z","shell.execute_reply":"2025-06-19T13:19:50.056272Z"}},"outputs":[{"name":"stdout","text":"\nStep 9/9: Saving the model...\nModel saved locally to 'Gemma-3-Tutor-local'\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Saved model to https://huggingface.co/Gemma-3-Emotion-Sensitive-Tutor-v1\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Model pushed to Hugging Face Hub as 'Gemma-3-Emotion-Sensitive-Tutor-v1'\n\n🎉 All steps completed! 🎉\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}