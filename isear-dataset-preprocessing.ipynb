{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1239644,"sourceType":"datasetVersion","datasetId":710787}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split, KFold\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom transformers import GPT2Tokenizer, GPT2ForSequenceClassification\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nimport itertools\n\nfrom torch import nn\nfrom transformers import BertModel, BertPreTrainedModel\nfrom transformers import BertConfig\nimport torch.nn.functional as F\nfrom torch.optim import Adam\n\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nfrom unicodedata import normalize\nfrom nltk.stem import SnowballStemmer\n\nimport random\nrn = random.uniform(0, 0.05)\n\n\ndef set_seed(seed):\n    random.seed(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     if torch.cuda.is_available():\n#         torch.cuda.manual_seed_all(seed)\n#         torch.backends.cudnn.deterministic = True\n#         torch.backends.cudnn.benchmark = False\n\n# Set seed for reproducibility\nset_seed(22)\n\nprint(\"Imported Packages...\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:39.257328Z","iopub.execute_input":"2024-09-23T07:27:39.25828Z","iopub.status.idle":"2024-09-23T07:27:39.268513Z","shell.execute_reply.started":"2024-09-23T07:27:39.258243Z","shell.execute_reply":"2024-09-23T07:27:39.267418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def preprocessing(data):\n    \"\"\"\n    Preprocess tweets by cleaning, removing stopwords, and stemming.\n    \n    Args:\n    - data (DataFrame): DataFrame containing tweets and sentiments.\n    \n    Returns:\n    - data (DataFrame): Preprocessed DataFrame with cleaned tweets.\n    \"\"\"\n    \n    tweets = []\n    sentiment = []\n\n    for index, tweet in data.iterrows():\n        words_cleaned = \"\"\n        tweet_clean = tweet.content.lower()\n    \n        words_cleaned = \" \".join([word for word in tweet_clean.split()\n                                  if 'http://' not in word\n                                  and 'https://' not in word\n                                  and '.com' not in word\n                                  and '.es' not in word\n                                  and word != 'rt'])\n        \n        # Remove only # and @ characters\n        tweet_clean = re.sub(r'[@#]', '', words_cleaned)\n        \n        # Perform additional cleaning steps\n        tweet_clean = re.sub(r'\\b([jh]*[aeiou]*[jh]+[aeiou]*)*\\b', \"\", tweet_clean)\n        tweet_clean = re.sub(r'(.)\\1{2,}', r'\\1', tweet_clean)\n        tweet_clean = re.sub(\n            r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", \n            normalize(\"NFD\", tweet_clean), 0, re.I)\n        tweet_clean = re.sub(\"[^a-zA-Z]\", \" \", tweet_clean)\n        tweet_clean = re.sub(\"\\t\", \" \", tweet_clean)\n        tweet_clean = re.sub(\" +\", \" \", tweet_clean) \n        tweet_clean = re.sub(\"^ \", \"\", tweet_clean)\n        tweet_clean = re.sub(\" $\", \"\", tweet_clean)\n        tweet_clean = re.sub(\"\\n\", \"\", tweet_clean)\n        \n        words_cleaned = \"\"\n        stemmed = \"\"\n        \n        stop_words = set(stopwords.words('english'))\n        stemmer = SnowballStemmer('english')\n        \n        tokens = word_tokenize(tweet_clean)\n        \n        words_cleaned = [word for word in tokens if word not in stop_words]\n        stemmed = \" \".join([stemmer.stem(word) for word in words_cleaned])\n        \n        sentiment.append(tweet.sentiment)\n        tweets.append(stemmed)\n    \n    data['Content'] = tweets\n    data['Sentiment'] = sentiment\n    data = data[['Sentiment', 'Content']]\n    \n    return data\n\nprint(\"Helper Functions loaded.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:39.742584Z","iopub.execute_input":"2024-09-23T07:27:39.743266Z","iopub.status.idle":"2024-09-23T07:27:39.75502Z","shell.execute_reply.started":"2024-09-23T07:27:39.743234Z","shell.execute_reply":"2024-09-23T07:27:39.754106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reading Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/isear-dataset/eng_dataset.csv\")\ndata.drop([\"ID\"], axis=1, inplace=True)\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:40.117762Z","iopub.execute_input":"2024-09-23T07:27:40.118145Z","iopub.status.idle":"2024-09-23T07:27:40.150414Z","shell.execute_reply.started":"2024-09-23T07:27:40.118116Z","shell.execute_reply":"2024-09-23T07:27:40.149516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentiment_counts = data.sentiment.value_counts()\nplt.figure(figsize=(8, 6))\nplt.bar(sentiment_counts.index, sentiment_counts.values, color='skyblue')\nplt.xlabel('Sentiment')\nplt.ylabel('Count')\nplt.title('Sentiment Distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:40.449626Z","iopub.execute_input":"2024-09-23T07:27:40.450283Z","iopub.status.idle":"2024-09-23T07:27:40.613854Z","shell.execute_reply.started":"2024-09-23T07:27:40.450251Z","shell.execute_reply":"2024-09-23T07:27:40.612951Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"words = []\nletters = []\n\nfor index, text in data.iterrows():\n    letters.append(len(text.content))\n    words.append(len(text.content.split()))\n\ndata['Words'] = words\ndata['Letters'] = letters\n\ngrouped_data = data.groupby('sentiment')\navg_words = grouped_data['Words'].mean()\navg_letters = grouped_data['Letters'].mean()\n\ngrouped_data = data.groupby('sentiment')\n\n# Boxplot for average number of characters per sentiment class\nfig, ax = plt.subplots(figsize=(6, 5))\nax.boxplot([group['Letters'] for name, group in grouped_data], labels=grouped_data.groups.keys())\nax.set_title(\"Number of Characters per Sentiment Class\")\nax.set_xlabel('Sentiment Class')\nax.set_ylabel('Number of Characters')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:40.615715Z","iopub.execute_input":"2024-09-23T07:27:40.616048Z","iopub.status.idle":"2024-09-23T07:27:41.328229Z","shell.execute_reply.started":"2024-09-23T07:27:40.616012Z","shell.execute_reply":"2024-09-23T07:27:41.327314Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Boxplot for average number of words per sentiment class\nfig, ax = plt.subplots(figsize=(6, 5))\nax.boxplot([group['Words'] for name, group in grouped_data], labels=grouped_data.groups.keys())\nax.set_title(\"Number of Words per Sentiment Class\")\nax.set_xlabel('Sentiment Class')\nax.set_ylabel('Number of Words')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:41.329886Z","iopub.execute_input":"2024-09-23T07:27:41.330164Z","iopub.status.idle":"2024-09-23T07:27:41.594621Z","shell.execute_reply.started":"2024-09-23T07:27:41.33014Z","shell.execute_reply":"2024-09-23T07:27:41.593743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Boxplot for characters distribution across all data\nfig, ax = plt.subplots(figsize=(6, 5))\nax.boxplot(data['Letters'])\nax.set_title(\"Overall Characters Distribution\")\nax.set_ylabel('Number of Characters')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:41.595695Z","iopub.execute_input":"2024-09-23T07:27:41.595966Z","iopub.status.idle":"2024-09-23T07:27:41.838181Z","shell.execute_reply.started":"2024-09-23T07:27:41.595941Z","shell.execute_reply":"2024-09-23T07:27:41.837243Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Boxplot for words distribution across all data\nfig, ax = plt.subplots(figsize=(6, 5))\nax.boxplot(data['Words'])\nax.set_title(\"Overall Words Distribution\")\nax.set_ylabel('Number of Words')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:41.840093Z","iopub.execute_input":"2024-09-23T07:27:41.840379Z","iopub.status.idle":"2024-09-23T07:27:42.067202Z","shell.execute_reply.started":"2024-09-23T07:27:41.840354Z","shell.execute_reply":"2024-09-23T07:27:42.066326Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Cleaning and Transformation","metadata":{}},{"cell_type":"code","source":"data_cleaned = preprocessing(data)\ndata_cleaned = data_cleaned.loc[:,['Sentiment','Content']]\ndata_copy = data_cleaned.copy()\ndata_cleaned.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:42.069145Z","iopub.execute_input":"2024-09-23T07:27:42.069573Z","iopub.status.idle":"2024-09-23T07:27:48.157408Z","shell.execute_reply.started":"2024-09-23T07:27:42.069539Z","shell.execute_reply":"2024-09-23T07:27:48.156365Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_mapping = {\"anger\": 0, \"fear\": 1, \"joy\": 2, \"sadness\": 3}\ndata_cleaned['Sentiment'] = data_cleaned['Sentiment'].map(label_mapping)\ndata_cleaned.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:48.159459Z","iopub.execute_input":"2024-09-23T07:27:48.160143Z","iopub.status.idle":"2024-09-23T07:27:48.171183Z","shell.execute_reply.started":"2024-09-23T07:27:48.160106Z","shell.execute_reply":"2024-09-23T07:27:48.170266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train, df_test = train_test_split(data_cleaned, test_size=0.2, random_state=22, stratify=data_cleaned['Sentiment'])\ndf_train1 = data_cleaned.copy()\nprint(f'Training set size: {df_train.shape[0]}')\nprint(f'Testing set size: {df_test.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:48.172465Z","iopub.execute_input":"2024-09-23T07:27:48.172781Z","iopub.status.idle":"2024-09-23T07:27:48.186827Z","shell.execute_reply.started":"2024-09-23T07:27:48.172756Z","shell.execute_reply":"2024-09-23T07:27:48.185947Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    dataset = SentimentDataset(\n        texts=df.Content.to_numpy(),\n        labels=df.Sentiment.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n    )\n    return DataLoader(dataset, batch_size=batch_size, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T07:27:48.1886Z","iopub.execute_input":"2024-09-23T07:27:48.18889Z","iopub.status.idle":"2024-09-23T07:27:48.19765Z","shell.execute_reply.started":"2024-09-23T07:27:48.188866Z","shell.execute_reply":"2024-09-23T07:27:48.196716Z"},"trusted":true},"outputs":[],"execution_count":null}]}